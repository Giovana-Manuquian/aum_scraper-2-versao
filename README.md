# ğŸš€ AUM Scraper - Coletor AutomÃ¡tico de PatrimÃ´nio Sob GestÃ£o

Sistema Full-Stack em Python 3.11 para coleta automÃ¡tica de PatrimÃ´nio Sob GestÃ£o (AUM) de empresas financeiras, utilizando web scraping inteligente e IA GPT-4o para extraÃ§Ã£o de dados.

## âœ¨ CaracterÃ­sticas Principais

- **Web Scraping Inteligente**: Usa Playwright para capturar conteÃºdo estÃ¡tico e dinÃ¢mico
- **IA GPT-4o**: ExtraÃ§Ã£o automÃ¡tica de AUM com controle de budget de tokens
- **Paralelismo Controlado**: Sistema de filas RabbitMQ para evitar bloqueios
- **PersistÃªncia Robusta**: PostgreSQL com SQLAlchemy 2 e Alembic
- **API REST Completa**: FastAPI com documentaÃ§Ã£o automÃ¡tica
- **Monitoramento**: Controle de uso de tokens e estatÃ­sticas de scraping
- **ExportaÃ§Ã£o**: RelatÃ³rios Excel com todos os dados coletados

## ğŸ—ï¸ Arquitetura

```
AUM_Scraper/
â”œâ”€â”€ backend/                 # Backend Python
â”‚   â”œâ”€â”€ app/                # AplicaÃ§Ã£o principal
â”‚   â”‚   â”œâ”€â”€ models/         # Modelos SQLAlchemy
â”‚   â”‚   â”œâ”€â”€ schemas/        # Schemas Pydantic
â”‚   â”‚   â”œâ”€â”€ services/       # ServiÃ§os de negÃ³cio
â”‚   â”‚   â””â”€â”€ main.py         # AplicaÃ§Ã£o FastAPI
â”‚   â”œâ”€â”€ tests/              # Testes Pytest
â”‚   â”œâ”€â”€ alembic/            # MigraÃ§Ãµes do banco
â”‚   â”œâ”€â”€ Dockerfile          # Container Docker
â”‚   â””â”€â”€ requirements.txt    # DependÃªncias Python
â”œâ”€â”€ data/                   # Dados e arquivos CSV
â””â”€â”€ docker-compose.yml      # OrquestraÃ§Ã£o Docker
```

## ğŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Docker e Docker Compose
- Python 3.11+ (para desenvolvimento local)
- OpenAI API Key

### 1. Clone o repositÃ³rio

```bash
git clone <repository-url>
cd AUM_Scraper
```

### 2. Configure as variÃ¡veis de ambiente

Crie um arquivo `.env` na raiz do projeto:

```bash
# OpenAI API
OPENAI_API_KEY=sua_chave_api_aqui

# Banco de dados
DATABASE_URL=postgresql://scraper:scraperpw@localhost:5432/scraperdb

# RabbitMQ
RABBITMQ_URL=amqp://guest:guest@localhost:5672/

# ConfiguraÃ§Ãµes da aplicaÃ§Ã£o
DEBUG=false
HOST=0.0.0.0
PORT=8000
```

### 3. Execute com Docker Compose

```bash
# Inicia todos os serviÃ§os
docker-compose up -d

# Verifica status
docker-compose ps

# Logs em tempo real
docker-compose logs -f backend
```

### 4. Acesse a aplicaÃ§Ã£o

- **API**: http://localhost:8000
- **DocumentaÃ§Ã£o**: http://localhost:8000/docs
- **RabbitMQ Management**: http://localhost:15672 (guest/guest)

## ğŸ“Š Uso da API

### Endpoints Principais

#### Empresas
- `POST /companies/` - Criar empresa
- `GET /companies/` - Listar empresas
- `GET /companies/{id}` - Obter empresa especÃ­fica
- `PUT /companies/{id}` - Atualizar empresa
- `DELETE /companies/{id}` - Remover empresa

#### Scraping
- `POST /companies/{id}/scrape` - Disparar scraping para empresa
- `POST /companies/bulk-scrape` - Scraping em lote
- `GET /scraping/status` - Status geral do scraping

#### AUM
- `GET /aum/` - Listar snapshots de AUM
- `GET /aum/latest` - AUM mais recente de cada empresa

#### Monitoramento
- `GET /usage/today` - Uso de tokens do dia
- `GET /usage/stats` - EstatÃ­sticas de uso
- `GET /queues/stats` - Status das filas

#### ExportaÃ§Ã£o
- `POST /export/excel` - Gerar relatÃ³rio Excel
- `GET /export/download/{filename}` - Download do arquivo

#### Upload
- `POST /upload/csv` - Importar empresas via CSV

### Exemplo de Uso

#### 1. Criar empresa

```bash
curl -X POST "http://localhost:8000/companies/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Banco Teste",
    "url_site": "https://bancoteste.com.br",
    "sector": "Bancos"
  }'
```

#### 2. Disparar scraping

```bash
curl -X POST "http://localhost:8000/companies/1/scrape"
```

#### 3. Verificar status

```bash
curl "http://localhost:8000/scraping/status"
```

#### 4. Exportar dados

```bash
curl -X POST "http://localhost:8000/export/excel"
```

## ğŸ“ Estrutura de Dados

### CSV de Entrada

O arquivo `data/companies.csv` deve conter:

```csv
name,url_site,url_linkedin,url_instagram,url_x,sector,employees_count
Empresa A,https://site.com,https://linkedin.com/empresa,https://instagram.com/empresa,https://x.com/empresa,Bancos,1000
```

### Tabelas do Banco

- **companies**: InformaÃ§Ãµes das empresas
- **scrape_logs**: Logs de scraping
- **aum_snapshots**: Snapshots de AUM coletados
- **usage**: Monitoramento de uso de tokens

## ğŸ”§ Desenvolvimento

### ConfiguraÃ§Ã£o do Ambiente Local

```bash
# Cria ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# Instala dependÃªncias
pip install -r backend/requirements.txt

# Instala Playwright
playwright install

# Configura variÃ¡veis de ambiente
export OPENAI_API_KEY=sua_chave_aqui
export DATABASE_URL=postgresql://scraper:scraperpw@localhost:5432/scraperdb
```

### Executar Testes

```bash
# Todos os testes
pytest backend/tests/ -v

# Com cobertura
pytest backend/tests/ --cov=app --cov-report=html

# Teste especÃ­fico
pytest backend/tests/test_models.py::TestCompany::test_create_company -v
```

### MigraÃ§Ãµes do Banco

```bash
# Gerar migraÃ§Ã£o
alembic revision --autogenerate -m "DescriÃ§Ã£o da mudanÃ§a"

# Aplicar migraÃ§Ãµes
alembic upgrade head

# Reverter migraÃ§Ã£o
alembic downgrade -1
```

### Executar Localmente

```bash
# Inicia apenas o banco e RabbitMQ
docker-compose up db rabbitmq -d

# Executa a aplicaÃ§Ã£o
cd backend
python main.py
```

## ğŸ³ Docker

### ServiÃ§os

- **PostgreSQL 15**: Banco de dados principal
- **RabbitMQ**: Sistema de filas assÃ­ncronas
- **Backend**: AplicaÃ§Ã£o Python com FastAPI

### Comandos Ãšteis

```bash
# Rebuild da imagem
docker-compose build --no-cache

# Logs especÃ­ficos
docker-compose logs backend

# Executar comando no container
docker-compose exec backend python -c "print('Hello World')"

# Parar todos os serviÃ§os
docker-compose down

# Parar e remover volumes
docker-compose down -v
```

## ğŸ“ˆ Monitoramento e Logs

### Logs da AplicaÃ§Ã£o

```bash
# Logs em tempo real
docker-compose logs -f backend

# Logs especÃ­ficos
docker-compose logs backend | grep "ERROR"
```

### MÃ©tricas de Uso

- **Tokens OpenAI**: Monitoramento automÃ¡tico de budget
- **Status de Scraping**: Taxa de sucesso, bloqueios, erros
- **Filas RabbitMQ**: NÃºmero de mensagens, consumidores

### Alertas

- Budget de tokens > 80% do limite diÃ¡rio
- Taxa de sucesso de scraping < 50%
- Filas com muitas mensagens pendentes

## ğŸ”’ SeguranÃ§a

- UsuÃ¡rio nÃ£o-root no container Docker
- ValidaÃ§Ã£o de entrada com Pydantic
- SanitizaÃ§Ã£o de URLs e conteÃºdo
- Rate limiting nas filas
- Logs de auditoria para todas as operaÃ§Ãµes

## ğŸš¨ Troubleshooting

### Problemas Comuns

#### 1. Erro de conexÃ£o com banco

```bash
# Verifica se o PostgreSQL estÃ¡ rodando
docker-compose ps db

# Testa conexÃ£o
docker-compose exec backend python -c "
from app.models.database import engine
print(engine.execute('SELECT 1').scalar())
"
```

#### 2. Erro de Playwright

```bash
# Reinstala browsers
docker-compose exec backend playwright install --with-deps chromium

# Verifica dependÃªncias do sistema
docker-compose exec backend ldd /usr/bin/chromium
```

#### 3. Erro de RabbitMQ

```bash
# Verifica status
docker-compose exec rabbitmq rabbitmqctl status

# Reinicia serviÃ§o
docker-compose restart rabbitmq
```

#### 4. Erro de OpenAI

```bash
# Verifica API key
echo $OPENAI_API_KEY

# Testa conexÃ£o
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models
```

## ğŸ“ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ¤ Suporte

- **Issues**: Use o GitHub Issues para reportar bugs
- **DocumentaÃ§Ã£o**: Consulte `/docs` na API para documentaÃ§Ã£o interativa
- **Logs**: Verifique logs do container para debugging

## ğŸ”® Roadmap

- [ ] Interface web para administraÃ§Ã£o
- [ ] Sistema de notificaÃ§Ãµes por email
- [ ] IntegraÃ§Ã£o com mais fontes de dados
- [ ] Machine Learning para melhorar extraÃ§Ã£o
- [ ] Dashboard de mÃ©tricas em tempo real
- [ ] API para terceiros
- [ ] Sistema de backup automÃ¡tico

---

**Desenvolvido com â¤ï¸ para automatizar a coleta de dados financeiros**
